---
title: "ATL Dorling Cartogram Testing"
output: html_document
date: "2025-11-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

# packages

library( geojsonio )   # read shapefiles
library( sp )          # work with shapefiles
library( sf )          # work with shapefiles - simple features format
library( mclust )      # cluster analysis 
library( tmap )        # theme maps
library( ggplot2 )     # graphing 
library( ggthemes )    # nice formats for ggplots
library( dplyr )       # data wrangling 
library( pander )      # formatting RMD tables
library( tidycensus )

library( cartogram )  # spatial maps w/ tract size bias reduction

```

 
## Data Source

We will again use Census data made available through the Diversity and Disparities Project. They have created a dataset with 170 variables that have been harmonized from 1970 onward for analysis of changes in tracts over time. We will use a subset of these:

## Build Your Rodeo Dataset

The most time-intensive component of most data projects the wrangling necessary to combine data from a variety of sources, cleaning it, transforming variables, and standardizing the data before it is ready for formal models.

We will use clustering to identify coherent neighborhoods within cities, but we first must acquire data, download shapefiles, transform the shapefiles into Dorling cartograms, and merge all of the map and census files together for the analysis.

This lab requires the following steps:

    Select a city (MSA) and identify all counties in the city
    Download a shapefile for your city
    Merge census data to your map file
    Transform the shapefile into a Dorling cartogram for more meaningful maps

At this point you are ready to start clustering census tracts.

## Step 1: Select Atlanta

To get Census data on the city you will first need to identify all of the counties that comprise the MSA. You can look this information up through MSA to FIPS crosswalks provided by the National Bureau for Economic Research (NBER): https://www.nber.org/data/cbsa-fips-county-crosswalk.html

I have added the file to GitHub for ease of access.


```{r}

crosswalk <- read.csv( "https://raw.githubusercontent.com/DS4PS/cpp-529-master/master/data/cbsatocountycrosswalk.csv",  stringsAsFactors=F, colClasses="character" )

grep( "^ATL", crosswalk$msaname, value=TRUE ) 

```

Select all of your county fips. To use them in the TidyCenss package you will need to split the state and county:

```{r}

these.atl <- crosswalk$msaname == "ATLANTA, GA"
these.fips <- crosswalk$fipscounty[ these.atl ]
these.fips <- na.omit( these.fips )

```


## Step 2: Download a Shapefile with Population Data

To create a Dorling cartogram we need a shapefile and a population count. We can get both through the Census download that includes simple features.


```{r}

#library( tidycensus )

census_api_key("35587aff2e52d96d0c22ac617d3713db90cb7aa7", install = TRUE, overwrite=TRUE )
readRenviron("~/.Renviron")

```


```{r, results='hide'}

these.atl <- crosswalk$msaname == "ATLANTA, GA"
these.fips <- crosswalk$fipscounty[ these.atl ]
these.fips <- na.omit( these.fips )

state.fips <- substr( these.fips, 1, 2 )
county.fips <- substr( these.fips, 3, 5 )

# cbind( these.fips, state.fips, county.fips ) %>% 
#   pander()



atl.pop <-
get_acs( geography = "tract", 
         variables = "B01003_001",
         state = "13", 
         county = county.fips[state.fips=="13"], 
         geometry = TRUE ) %>% 
         select( GEOID, estimate ) %>%
         rename( POP = estimate )  ## there was an error: Error in rename(., POP = estimate) : unused argument (POP = estimate)// so I added "dplyr::" to rename()

```
## Step 3: Add Census Data


```{r}

URL <- "https://github.com/DS4PS/cpp-529-master/raw/master/data/ltdb_std_2010_sample.rds"
census.dat <- readRDS(gzcon(url( URL )))

# can merge an sf object and data.frame
atl <- merge( atl.pop, census.dat, by.x="GEOID", by.y="tractid" )

# make sure there are no empty polygons
atl <- atl[ ! st_is_empty( atl ) , ]

```


**DATA DICTIONARY**


```{r, echo=F}
data.dictionary <- 
structure(list(
  
LABEL = c("tractid", "pnhwht12", "pnhblk12", "phisp12", "pasian12",
"pntv12", "pfb12", "polang12", "phs12", "pcol12", "punemp12", 
"pflabf12", "pprof12", "pmanuf12", "pvet12", "psemp12", "hinc12", 
"incpc12", "ppov12", "pown12", "pvac12", "pmulti12", "mrent12", 
"mhmval12", "p30old12", "p10yrs12", "p18und12", "p60up12", "p75up12", 
"pmar12", "pwds12", "pfhh12"), 

VARIABLE = c("GEOID", "Percent white, non-Hispanic", 
"Percent black, non-Hispanic", "Percent Hispanic", "Percent Asian Population", "Percent Native American race", 
"Percent foreign born", "Percent speaking other language at home, age 5 plus", 
"Percent with high school degree or less", "Percent with 4-year college degree or more", 
"Percent unemployed", "Percent female labor force participation", 
"Percent professional employees", "Percent manufacturing employees", 
"Percent veteran", "Percent self-employed", "Median HH income, total", 
"Per capita income", "Percent in poverty, total", "Percent owner-occupied units", 
"Percent vacant units", "Percent multi-family units", "Median rent", 
"Median home value", "Percent structures more than 30 years old", 
"Percent HH in neighborhood 10 years or less", "Percent 17 and under, total", 
"Percent 60 and older, total", "Percent 75 and older, total", 
"Percent currently married, not separated", "Percent widowed, divorced and separated", 
"Percent female-headed families with children")), class = "data.frame", row.names = c(NA, 
-31L))

# data.dictionary %>% pander()
```


## Step 4: Transform the Shapefile into A Dorling Cartogram

```{r}

# convert sf map object to an sp version
atl.sp <- as_Spatial( atl )

#class( atl.sp )

## Here is where the M7 Dashboard Template, Create Your Dorling Cartogram picks up

# project map and remove empty tracts
atl.sp <- spTransform( atl.sp, CRS("+init=epsg:3395"))
atl.sp <- atl.sp[ atl.sp$POP != 0 & (! is.na( atl.sp$POP )) , ]

# convert census tract polygons to dorling cartogram
# no idea why k=0.03 works, but it does - default is k=5
atl.sp$pop.w <- atl.sp$POP / 9000        # standardizes it to max of 1.5
atl_dorling <- cartogram_dorling( x=atl.sp, weight="pop.w", k=0.05 )                   ## first instance of atl_dorling

d1 <- atl_dorling@data                   ## create first dataset, d1, with dorling data

# atl_dorling <- st_as_sf(atl_dorling) #added fix from Ana on lab 3, seems to fix the "geometry column not found" error
# 
# # Bounding Box
# # Improve the aestetics with a bounding box. I use the locator() function to get location.
# # https://youtu.be/jx5y0EdIUT4
# 
# # user-defined bounding box to move slocer to subjects 
# bb <- st_bbox( c( xmin =  -9447112, xmax = -9327980, 
#                   ymax = 4048483, ymin = 3921191 ), 
#                crs = st_crs("+init=epsg:3395"))
# 
# tm_shape( atl_dorling, bbox=bb ) + 
#   tm_polygons( col="hinc12", n=10, style="quantile", palette="Spectral" ) +
#   tm_layout( "Dorling Cartogram", title.position=c("right","top") )

```


## Prepare Data for Clustering

We will use the same set of variables as last week. The data is transformed into z-score so that they are all on similar scales.

We transform all of the variables to z scores so they are on the same scale while clustering. This ensures that each census variable has equal weight. Z-scores typically range from about -3 to +3 with a mean of zero.

```{r}

# keep.these <- c("pnhwht12", "pnhblk12", "phisp12", "pasian12", "pntv12", "pfb12", "polang12", 
#                 "phs12", "pcol12", "punemp12", "pflabf12", "pprof12", "pmanuf12", 
#                 "pvet12", "psemp12", "hinc12", "incpc12", "ppov12", "pown12", 
#                 "pvac12", "pmulti12", "mrent12", "mhmval12", "p30old12", "p10yrs12", 
#                 "p18und12", "p60up12", "p75up12", "pmar12", "pwds12", "pfhh12")
# 
# atl_dorling <- as_Spatial(atl_dorling) # converts atl_dorling from sf to sp
# 
# d1 <- atl_dorling@data
# d2 <- select( d1, all_of(keep.these) )
# d3 <- apply( d2, 2, scale ) 
# head( d3[,1:6] ) %>% pander() 



keep.these <- c("pnhwht12", "pnhblk12", "phisp12", "pasian12", "pntv12", "pfb12", "polang12", 
"phs12", "pcol12", "punemp12", "pflabf12", "pprof12", "pmanuf12", 
"pvet12", "psemp12", "hinc12", "incpc12", "ppov12", "pown12", 
"pvac12", "pmulti12", "mrent12", "mhmval12", "p30old12", "p10yrs12", 
"p18und12", "p60up12", "p75up12", "pmar12", "pwds12", "pfhh12")

d2 <- select( d1, keep.these )
d3 <- apply( d2, 2, scale )
head( d3[,1:6] ) %>% pander()

```

## Prepare Data for Clustering

We transform all of the variables to z scorse so they are on the same scale while clustering. This ensures that each census variable has equal weight. Z-scores typically range from about -3 to +3 with a mean of zero.

```{r}
keep.these <- c("pnhwht12", "pnhblk12", "phisp12", "pasian12", "pntv12", "pfb12", "polang12", 
"phs12", "pcol12", "punemp12", "pflabf12", "pprof12", "pmanuf12", 
"pvet12", "psemp12", "hinc12", "incpc12", "ppov12", "pown12", 
"pvac12", "pmulti12", "mrent12", "mhmval12", "p30old12", "p10yrs12", 
"p18und12", "p60up12", "p75up12", "pmar12", "pwds12", "pfhh12")

d2 <- select( d1, keep.these )
d3 <- apply( d2, 2, scale )
```

## Perform Cluster Analysis
 
For more details on cluster analysis visit the mclust tutorial. 
 
```{r}

# library( mclust )
set.seed( 1234 )
fit <- Mclust( d3 )
atl_dorling$cluster <- as.factor( fit$classification )
summary( fit )

```

## Identifying Neighborhood Clusters

Build the charts to compare census characteristics across the groups. 

```{r}

df.pct <- sapply( d2, ntile, 100 )
d4 <- as.data.frame( df.pct )
d4$cluster <- as.factor( paste0("GROUP-",fit$classification) )

num.groups <- length( unique( fit$classification ) )

stats <- 
d4 %>% 
  group_by( cluster ) %>% 
  summarise_each( funs(mean) )

t <- data.frame( t(stats), stringsAsFactors=F )
names(t) <- paste0( "GROUP.", 1:num.groups )
t <- t[-1,]


## after adding in "pasian12" to the data dictionary, I also had to make sure the x,y coord matched. from 1:30 -> 1:31
for( i in 1:num.groups )
{
  z <- t[,i]
  plot( rep(1,31), 1:31, bty="n", xlim=c(-75,100), 
        type="n", xaxt="n", yaxt="n",
        xlab="Percentile", ylab="",
        main=paste("GROUP",i) )
  abline( v=seq(0,100,25), lty=3, lwd=1.5, col="gray90" )
  segments( y0=1:31, x0=0, x1=100, col="gray70", lwd=2 )
  text( -0.2, 1:31, data.dictionary$VARIABLE[-1], cex=0.85, pos=2 )
  points( z, 1:31, pch=19, col="firebrick", cex=1.5 )
  axis( side=1, at=c(0,50,100), col.axis="gray", col="gray" )
}

```

You are now ready to identify meaningful labels for your clusters!

```{r}

## adding in this code block to see if I can create a cluster map from my first Perform Cluster Analysis block
## this should serve to accomplish the "add clusters" step of M7 Dashboard Template overview

# library(sf)
# library(tmap)

# Reproject phx and convert to sf
atl_dorling <- st_as_sf(atl_dorling)  # in case it's not already an sf object
atl_dorling <- st_transform(atl_dorling, crs = 3395)  # use EPSG:3395 directly

tmap_mode("plot")
tmap_style("cobalt")

# user-defined bounding box to move slocer to subjects 
bb <- st_bbox( c( xmin =  -9447112, xmax = -9327980, 
                  ymax = 4048483, ymin = 3921191 ), 
               crs = st_crs("+init=epsg:3395"))

tm1 <- 
tm_shape( atl_dorling, bbox=bb ) + 
  tm_polygons( col="cluster", palette="Accent"  )


tmap_arrange( tm1)

```

-------------------------------------- At this point, the code works to create a dorling cartogram .geojson file -------------

#Add Census Data

Add all of the census data to your shapefile:

```{r}

URL1 <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-2000.rds"
d5 <- readRDS( gzcon( url( URL1 ) ) )

URL2 <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-2010.rds"
d6 <- readRDS( gzcon( url( URL2 ) ) )

URLmd <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-META-DATA.rds"
md <- readRDS( gzcon( url( URLmd ) ) )

d5 <- select( d5, - year )
d6 <- select( d6, - year )

d7 <- merge( d5, d6, by="tractid" )
d7 <- merge( d7, md, by="tractid" )

# Drop all rural census tracts:

table( d7$urban )

d7 <- filter( d7, urban == "urban" )

# Create a variable that measures the growth of median home value from 2000 to 2010:

d8 <- select( d7, tractid, 
             mhmval00, mhmval12, 
             hinc00, 
             hu00, vac00, own00, rent00, h30old00,
             empclf00, clf00, unemp00, prof00,  
             dpov00, npov00,
             ag25up00, hs00, col00, 
             pop00.x, nhwht00, nhblk00, hisp00, asian00, 
             cbsa, cbsaname )

 
d8 <- 
  d8 %>%
  mutate( # percent white in 2000
          p.white = 100 * nhwht00 / pop00.x,
          # percent black in 2000
          p.black = 100 * nhblk00 / pop00.x,
          # percent hispanic in 2000
          p.hisp = 100 * hisp00 / pop00.x, 
          # percent asian in 2000
          p.asian = 100 * asian00 / pop00.x,
          # percent high school grads by age 25 in 2000 
          p.hs = 100 * (hs00+col00) / ag25up00,
          # percent pop with college degree in 2000
          p.col = 100 * col00 / ag25up00,
          # percent employed in professional fields in 2000
          p.prof = 100 * prof00 / empclf00,
          # percent unemployment  in 2000
          p.unemp = 100 * unemp00 / clf00,
          # percent of housing lots in tract that are vacant in 2000
          p.vacant = 100 * vac00 / hu00,
          # percent of homeowners in 2000
          p.own = 100 * own00 / pop00.x,
          # dollar change in median home value 2000 to 2010 
          pov.rate = 100 * npov00 / dpov00)


# adjust 2000 home values for inflation 
mhv.00 <- d8$mhmval00 * 1.28855  
mhv.10 <- d8$mhmval12

# change in MHV in dollars
mhv.change <- mhv.10 - mhv.00


# drop low 2000 median home values
# to avoid unrealistic growth rates.

# tracts with homes that cost less than
# $1,000 are outliers
mhv.00[ mhv.00 < 1000 ] <- NA

# change in MHV in percent
mhv.growth <- 100 * ( mhv.change / mhv.00 )

d8$mhv.00 <- mhv.00
d8$mhv.10 <- mhv.10
d8$mhv.change <- mhv.change
d8$mhv.growth <- mhv.growth 

# filter out any homes with more than 200% growth

d8 <- d8 %>% 
  filter(mhv.growth < 200)

# STANDARDIZE GEO IDs

# note the current geoid format for the LTDB census data: 
# FIPS-STATE-COUNTY-TRACT:  fips-01-001-020100  

x <- d8$tractid 
# head( x )
# [1] "fips-01-001-020100" "fips-01-001-020200" "fips-01-001-020300"
# [4] "fips-01-001-020400" "fips-01-001-020500" "fips-01-001-020600"

 # remove non-numeric strings
 x <- gsub( "fips", "", x )
 x <- gsub( "-", "", x )
 # head( x )
 # [1] "01001020100" "01001020200" "01001020300" "01001020400" "01001020500"
 # [6] "01001020600"

 # drop leading zeros
 x <- as.numeric( x )

# remember to add the variable back to the census dataset
d8$tractid2 <- x      ## note that it is tractid2, not tractid
          
# can merge an sf object and data.frame
# this does duplicate columns, but as long as i add ".x" after every variable, it's fine...annoying, but fine.
## somehow, adding the homevalues data fixed a majority of this. now only one duplicate column for mhmval12, I'll take it.
atl <- merge( atl_dorling, d8, by.x="GEOID", by.y="tractid2")       ## remember to add a 2 to the tractid to get "by.y="tractid2""


```





```{r}

#library( geojsonio ) 

atl <- as_Spatial( atl )

# data frame and polygon ID standardization in case a tract was dropped and IDs don't match
row.ids <- sapply( slot( atl, "polygons" ), function(x) slot( x, "ID" ) )
row.names( atl ) <- row.ids

# project to standard lat-lon coordinate system 
atl <- spTransform( atl, CRS("+proj=longlat +datum=WGS84") )

# write to file 
geojson_write( atl, file="atl_dorling_6.geojson", geometry="polygon" )

```


